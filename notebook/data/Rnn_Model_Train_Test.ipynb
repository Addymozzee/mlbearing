{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Aeesha\\\\DissProject\\\\mlbearing')\n",
    "import time\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report\n",
    "from sklearn import preprocessing\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "from src.utils import save_object\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "# class ModelTrainerConfig:\n",
    "#     trained_model_file_path: str = os.path.join(\"artifacts\", \"model.pkl\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        # self.model_trainer_config = ModelTrainerConfig()\n",
    "        self.B1, self.B2, self.B3, self.B4 = self._initialize_dicts()\n",
    "\n",
    "    def _initialize_dicts(self):\n",
    "        B1 ={\n",
    "            \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-23 09:14:13\"],\n",
    "            \"suspect\" : [\"2003-10-23 09:24:13\" , \"2003-11-08 12:11:44\"],\n",
    "            \"normal\" : [\"2003-11-08 12:21:44\" , \"2003-11-19 21:06:07\"],\n",
    "            \"suspect_1\" : [\"2003-11-19 21:16:07\" , \"2003-11-24 20:47:32\"],\n",
    "            \"imminent_failure\" : [\"2003-11-24 20:57:32\",\"2003-11-25 23:39:56\"]\n",
    "        }\n",
    "        B2 = {\n",
    "            \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "            \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-24 01:01:24\"],\n",
    "            \"suspect\" : [\"2003-11-24 01:11:24\" , \"2003-11-25 10:47:32\"],\n",
    "            \"imminient_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "        }\n",
    "\n",
    "        B3 = {\n",
    "            \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "            \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-22 09:16:56\"],\n",
    "            \"suspect\" : [\"2003-11-22 09:26:56\" , \"2003-11-25 10:47:32\"],\n",
    "            \"Inner_race_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "        }\n",
    "\n",
    "        B4 = {\n",
    "            \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-29 21:39:46\"],\n",
    "            \"normal\" : [\"2003-10-29 21:49:46\" , \"2003-11-15 05:08:46\"],\n",
    "            \"suspect\" : [\"2003-11-15 05:18:46\" , \"2003-11-18 19:12:30\"],\n",
    "            \"Rolling_element_failure\" : [\"2003-11-19 09:06:09\" , \"2003-11-22 17:36:56\"],\n",
    "            \"Stage_two_failure\" : [\"2003-11-22 17:46:56\" , \"2003-11-25 23:39:56\"]\n",
    "        }  # the B4 dictionary\n",
    "        \n",
    "        return B1, B2, B3, B4\n",
    "\n",
    "    def _get_state(self, time, dates_dict):\n",
    "        # Convert string time to datetime object for comparison\n",
    "        time = datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "        for state, date_ranges in dates_dict.items():\n",
    "            start_date = datetime.strptime(date_ranges[0], '%Y-%m-%d %H:%M:%S')\n",
    "            end_date = datetime.strptime(date_ranges[1], '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Check if the given time falls between start_date and end_date\n",
    "            if start_date <= time <= end_date:\n",
    "                return state\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def _assign_states(self, df):\n",
    "        for col, dates_dict in zip([\"B1_state\", \"B2_state\", \"B3_state\", \"B4_state\"], [self.B1, self.B2, self.B3, self.B4]):\n",
    "            df[col] = df.index.map(lambda x: self._get_state(x, dates_dict))\n",
    "                    \n",
    "        return df\n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        try:\n",
    "            logging.info(\"Split training and test input data\")\n",
    "            df = pd.read_csv('./notebook/data/set1_timefeatures.csv')\n",
    "            set1 = df.rename(columns={'Unnamed: 0':'time'}).set_index('time')\n",
    "            set1 = self._assign_states(set1)\n",
    "            \n",
    "            # Refactoring the loop for code reusability\n",
    "            state_configs = {\n",
    "                \"B1_state\": [(151, \"early\"), (600, \"suspect\"), (1499, \"normal\"), (2098, \"suspect\"), (2156, \"imminent_failure\")],\n",
    "                \"B2_state\": [(500, \"early\"), (2000, \"normal\"), (2120, \"suspect\"), (2156, \"imminent_failure\")],\n",
    "                \"B3_state\": [(500, \"early\"), (1790, \"normal\"), (2120, \"suspect\"), (2156, \"Inner_race_failure\")],\n",
    "                \"B4_state\": [(200, \"early\"), (1000, \"normal\"), (1435, \"suspect\"), (1840, \"Inner_race_failure\"), (2156, \"Stage_two_failure\")]\n",
    "            }\n",
    "\n",
    "            # Initializing states for each column based on counter\n",
    "            for state_col, config in state_configs.items():\n",
    "                state_list = list()\n",
    "                for limit, state_name in config:\n",
    "                    state_list.extend([state_name] * (limit - len(state_list)))\n",
    "                set1[state_col] = state_list + [None] * (len(set1) - len(state_list))\n",
    "\n",
    "            # Handle None values, you can choose to replace them or perform any other operation\n",
    "            set1 = set1.fillna(method='ffill')\n",
    "\n",
    "                            \n",
    "            B1_cols = [col for col in set1.columns if \"B1\" in col]\n",
    "            B2_cols = [col for col in set1.columns if \"B2\" in col]\n",
    "            B3_cols = [col for col in set1.columns if \"B3\" in col]\n",
    "            B4_cols = [col for col in set1.columns if \"B4\" in col]\n",
    "\n",
    "            B1 = set1[B1_cols]\n",
    "            B2 = set1[B2_cols]\n",
    "            B3 = set1[B3_cols]\n",
    "            B4 = set1[B4_cols]\n",
    "            cols = ['Bx_mean','Bx_std','Bx_skew','Bx_kurtosis','Bx_entropy','Bx_rms','Bx_max','Bx_p2p','Bx_crest', 'Bx_clearence', 'Bx_shape', 'Bx_impulse',\n",
    "                        'By_mean','By_std','By_skew','By_kurtosis','By_entropy','By_rms','By_max','By_p2p','By_crest', 'By_clearence', 'By_shape', 'By_impulse',\n",
    "                        'class']\n",
    "            # cols = [... ]  # The same list of column names as before\n",
    "            B1.columns = cols\n",
    "            B2.columns = cols\n",
    "            B3.columns = cols\n",
    "            B4.columns = cols\n",
    "            final_data = pd.concat([B1, B2, B3, B4], axis=0, ignore_index=True)\n",
    "\n",
    "            X = final_data.copy()\n",
    "            y = X.pop(\"class\")\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "            \n",
    "            # Reshape the data for LSTM\n",
    "            X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "            X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "            return(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(str(e))\n",
    "\n",
    "        # For RNN \n",
    "        # Modifying the objective_lstm function\n",
    "\n",
    "    def objective_lstm(self, trial):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = self.initiate_model_trainer()\n",
    "\n",
    "        n_units = trial.suggest_int(\"n_units\", 16, 128, log=True)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(units=n_units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "            tf.keras.layers.LSTM(units=n_units),\n",
    "            tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "            tf.keras.layers.Dense(units=len(np.unique(y_train)), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "\n",
    "        end_time = time.time()\n",
    "        self.log_and_print(\"LSTM\", accuracy, end_time - start_time)\n",
    "\n",
    "        # Compute ROC curve and ROC area\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        # Compute Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        avg_precision = average_precision_score(y_test, y_pred)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, color='b', lw=2, label='Precision-Recall curve (area = %0.2f)' % avg_precision)\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.show()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def log_and_print(self, model_name, score, duration):\n",
    "        print(f\"Model {model_name} found with score: {score:.4f}\")\n",
    "        print(f\"Model {model_name} took {duration:.2f} seconds to train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vibd = ModelTrainer()\n",
    "\n",
    "# Optimize for LSTM\n",
    "study_lstm = optuna.create_study(direction='maximize')\n",
    "study_lstm.optimize(vibd.objective_lstm, n_trials=20)  # 20 trials, adjust this as needed\n",
    "\n",
    "print(\"\\n--- LSTM Results ---\")\n",
    "print(\"Best Parameters: \", study_lstm.best_params)\n",
    "print(\"Best Score: \", study_lstm.best_value)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
